{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "# Set the target folder name you want to reach\n",
    "target_folder = \"phate-for-text\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Loop to move up the directory tree until we reach the target folder\n",
    "while os.path.basename(current_dir) != target_folder:\n",
    "    parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "    if parent_dir == current_dir:\n",
    "        # If we reach the root directory and haven't found the target, exit\n",
    "        raise FileNotFoundError(f\"{target_folder} not found in the directory tree.\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "# Change the working directory to the folder where \"phate-for-text\" is found\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Add the \"phate-for-text\" directory to sys.path\n",
    "sys.path.insert(0, current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filename_info(filename):\n",
    "    pattern = (\n",
    "        r\"results_all_methods_(?P<theme>.+)_hierarchy_t(?P<t>[\\d\\.]+)_\"\n",
    "        r\"maxsub(?P<max_sub>\\d+)_depth(?P<depth>\\d+)\"\n",
    "        r\"(?:_synonyms(?P<synonyms>\\d+))?\"\n",
    "        r\"(?:_noise(?P<noise>[\\d\\.]+))?\"\n",
    "        r\"(?:_(?P<branching>increasing|decreasing|constant|random))?\"\n",
    "        r\"\\.csv$\"\n",
    "    )\n",
    "\n",
    "    match = re.match(pattern, filename)\n",
    "    \n",
    "    if match:\n",
    "        info = match.groupdict()\n",
    "        info[\"synonyms\"] = info[\"synonyms\"] if info[\"synonyms\"] else \"0\"\n",
    "        info[\"noise\"] = info[\"noise\"] if info[\"noise\"] else \"0\"\n",
    "        info[\"branching\"] = info[\"branching\"] if info[\"branching\"] else \"constant\"\n",
    "        return info\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_files(folder_path, string_filters):\n",
    "    param_str = [\"'k': 4\", \"'t': 2\", \"'alpha': 2\", \"'alpha': 8\",\"alpha_end\", \"t_end\", \"k_end\"]\n",
    "    \n",
    "    combined_df = pd.DataFrame()\n",
    "    processed_files = []\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".csv\") and file.startswith(\"results_all_methods_\"):\n",
    "            if not all(s in file for s in string_filters):\n",
    "                continue\n",
    "            if 'Indonesia' in file:\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            file_info = extract_filename_info(file)\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Filter out rows based on undesired 'cluster_params' values\n",
    "            if \"cluster_params\" in df.columns:\n",
    "                df = df[~df[\"cluster_params\"].astype(str).apply(lambda param: any(p in param for p in param_str))]\n",
    "\n",
    "            # Create 'cluster_level' column based on ordinal encoding of 'level'\n",
    "            if \"level\" in df.columns:\n",
    "                unique_levels = sorted(df[\"level\"].unique())\n",
    "                level_to_cluster = {lvl: i + 1 for i, lvl in enumerate(unique_levels)}\n",
    "                df[\"level\"] = df[\"level\"].map(level_to_cluster)\n",
    "\n",
    "            # Add extracted filename info as new columns\n",
    "            for key, value in file_info.items():\n",
    "                df[key] = value\n",
    "            \n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            processed_files.append(file)\n",
    "\n",
    "    output_filename = \"processed_results_\" + \"_\".join(string_filters) + \".csv\"\n",
    "    output_path = os.path.join(folder_path, output_filename)\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    return combined_df, processed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage example\n",
    "embedding = 'text-embedding-3-large'\n",
    "folder_path = f\"{embedding}_results\"  # Change this to the actual folder path\n",
    "# second argument is just any strings to identify which datasets you want to merge\n",
    "combined_df, processed_files = process_files(folder_path,['Offshore energy impacts on fisheries','t1.0','maxsub5_depth3','random'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['maxsub5_depth3','maxsub3_depth5']:\n",
    "    for j in ['Offshore energy impacts on fisheries','Energy, Ecosystems, and Humans']:\n",
    "        combined_df, processed_files = process_files(folder_path,[j,'t1.0',i,'random'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
