{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "# Set the target folder name you want to reach\n",
    "target_folder = \"phate-for-text\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Loop to move up the directory tree until we reach the target folder\n",
    "while os.path.basename(current_dir) != target_folder:\n",
    "    parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "    if parent_dir == current_dir:\n",
    "        # If we reach the root directory and haven't found the target, exit\n",
    "        raise FileNotFoundError(f\"{target_folder} not found in the directory tree.\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "# Change the working directory to the folder where \"phate-for-text\" is found\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Add the \"phate-for-text\" directory to sys.path\n",
    "sys.path.insert(0, current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Energy, Ecosystems, and Humans\" \"Offshore energy impacts on fisheries\" \"West Java, Indonesia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None indicates all posibilities for that data parameter are included\n",
    "\n",
    "theme =None \n",
    "t=1.0\n",
    "maxsub = 5\n",
    "depth = 3\n",
    "synonyms= None\n",
    "noise=None\n",
    "branching = 'random'\n",
    "param_lst = {'theme':theme,'t':t,'maxsub':maxsub,'depth':depth,'synonyms':synonyms,'noise':noise,'':branching}\n",
    "\n",
    "embedding = 'text-embedding-3-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if None not in param_lst.values():\n",
    "    topic_data = pd.read_csv(f'data_generation/generated_data/{theme}_hierarchy_t{t}_maxsub{maxsub}_depth{depth}_synonyms{synonyms}_noise{noise}_{branching}.csv')\n",
    "    num_seed_topics = len(topic_data['category 0'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if None not in param_lst.values():\n",
    "    bertopic_file = f'{embedding}_results/results_all_methods_{theme}_hierarchy_t{t}_maxsub{maxsub}_depth{depth}_synonyms{synonyms}_{branching}.csv'\n",
    "else:\n",
    "    file_string = \"processed_results\"\n",
    "    for key,val in param_lst.items():\n",
    "        if val is not None:\n",
    "            if key !='theme':\n",
    "                file_string += f'_{key}{val}'\n",
    "            else:\n",
    "                file_string += f'_{val}'\n",
    "\n",
    "    file_string+='.csv'\n",
    "    bertopic_file = f'{embedding}_results/'+file_string\n",
    "            \n",
    "bertopic_result_df=pd.read_csv(bertopic_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bertopic_result_df = bertopic_result_df[bertopic_result_df['reduction_params']!=\"{}\"]\n",
    "bertopic_result_df = bertopic_result_df[bertopic_result_df['cluster_params']!=\"{}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_result_df.rename(columns={\"t\": \"temp\"}, inplace=True) # avoid overlap between temepreature for gpt and phate parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute default parameters for analysis\n",
    "replacement_map = {\n",
    "    \"{}\": \"{'n_components': 3, 'min_dist': 0.1, 'n_neighbors': 15}\",\n",
    "    \"{'n_components': 300}\": \"{'n_components': 300, 'min_dist': 0.1, 'n_neighbors': 15}\",\n",
    "    \"{'n_components': 100}\": \"{'n_components': 100, 'min_dist': 0.1, 'n_neighbors': 15}\"\n",
    "}\n",
    "\n",
    "# Apply changes in-place to rows where reduction_method is 'UMAP'\n",
    "bertopic_result_df.loc[\n",
    "    bertopic_result_df['reduction_method'] == 'UMAP', 'reduction_params'\n",
    "] = bertopic_result_df.loc[\n",
    "    bertopic_result_df['reduction_method'] == 'UMAP', 'reduction_params'\n",
    "].apply(lambda x: replacement_map.get(str(x), x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(df, target_col, predictor_cols, n_estimators=100, random_state=42, plot_violin=False):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest model to evaluate feature importance and optimal value ranges.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame containing the data\n",
    "    - target_col: String, name of the target column\n",
    "    - predictor_cols: List of strings, names of predictor columns\n",
    "    - n_estimators: Number of trees in the Random Forest (default: 100)\n",
    "    - random_state: Random seed for reproducibility (default: 42)\n",
    "    - plot_violin: Boolean, whether to plot violin plots for each predictor (default: False)\n",
    "    \n",
    "    Returns:\n",
    "    - feature_importance: DataFrame with feature importance scores\n",
    "    - optimal_ranges: Dictionary mapping features to optimal value estimates\n",
    "    \"\"\"\n",
    "    # Keep NaNs in predictors but drop NaNs in the target column\n",
    "    df = df.dropna(subset=[target_col])\n",
    "    \n",
    "    # Separate categorical and numeric columns\n",
    "    categorical_cols = [col for col in predictor_cols if df[col].dtype == 'object']\n",
    "    numeric_cols = [col for col in predictor_cols if col not in categorical_cols]\n",
    "    # Copy dataframe for encoding\n",
    "    df_encoded = df.copy()\n",
    "    label_encoders = {}\n",
    "\n",
    "    # Encode categorical variables (treat NaNs as \"Missing\")\n",
    "    for col in categorical_cols:\n",
    "        df_encoded[col] = df_encoded[col].astype(str).fillna(\"Missing\")  # Convert NaN to string\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "\n",
    "    # Handle NaNs in numeric columns by replacing them with a placeholder (-99999)\n",
    "    df_encoded[numeric_cols] = df_encoded[numeric_cols].dropna()\n",
    "\n",
    "    # Define features and target\n",
    "    X = df_encoded[predictor_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # Compute feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "    plt.title('Feature Importance in Random Forest')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute optimal ranges for features\n",
    "    optimal_ranges = {}\n",
    "    optimal_scores = {}\n",
    "    for feature in predictor_cols:\n",
    "        feature_vals = np.sort(df_encoded[feature].dropna().unique())  # Sorted unique values from encoded df\n",
    "        if len(feature_vals) == 0:\n",
    "            optimal_ranges[feature] = None\n",
    "            continue\n",
    "        \n",
    "        pred_vals = [rf.predict(X[df_encoded[feature] == val]) for val in feature_vals]\n",
    "        max_pred_idx = np.argmax([np.mean(pred) for pred in pred_vals])\n",
    "        optimal_value = feature_vals[max_pred_idx]\n",
    "        optimal_score = np.max(pred_vals[max_pred_idx])\n",
    "        optimal_scores[feature] = optimal_score\n",
    "        \n",
    "\n",
    "        # Decode categorical variables back to original values\n",
    "        if feature in label_encoders:\n",
    "            optimal_value = label_encoders[feature].inverse_transform([optimal_value])[0]\n",
    "        \n",
    "        optimal_ranges[feature] = optimal_value\n",
    "    # Optionally plot violin plots\n",
    "    if plot_violin:\n",
    "        for feature in predictor_cols:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            if feature in categorical_cols:\n",
    "                df_plot = df.copy()\n",
    "                df_plot[feature] = df_plot[feature].astype(str).fillna(\"Missing\")  # Keep NaNs as \"Missing\"\n",
    "                sns.violinplot(x=df_plot[feature], y=target_col, data=df_plot)\n",
    "            else:\n",
    "                df_plot = df.copy()\n",
    "                df_plot[feature] = df_plot[feature].fillna('None')  # Keep NaNs explicitly\n",
    "                sns.violinplot(x=df_plot[feature].astype(str), y=target_col, data=df_plot)\n",
    "            \n",
    "            plt.title(f'Distribution of {target_col} by {feature}')\n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel(target_col)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "    \n",
    "    return feature_importance, optimal_ranges,optimal_scores\n",
    "\n",
    "def filter_and_expand(df, category, filter_val):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame based on a specific value in the given method column\n",
    "    and expands the dictionary in the corresponding params column into separate columns.\n",
    "    \n",
    "    :param df: DataFrame to process\n",
    "    :param category: Either 'reduction' or 'cluster' to specify which method to filter\n",
    "    :param filter_val: Value to filter for\n",
    "    :return: Processed DataFrame\n",
    "    \"\"\"\n",
    "    method_col = f\"{category}_method\"\n",
    "    params_col = f\"{category}_params\"\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df[method_col] == filter_val].copy()\n",
    "    \n",
    "    # Convert string dictionaries to actual dictionaries (if necessary)\n",
    "    if filtered_df[params_col].dtype == 'object':\n",
    "        filtered_df[params_col] = filtered_df[params_col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    # print(filtered_df[params_col])\n",
    "    # Expand the parameters dictionary into separate columns\n",
    "    params_df = filtered_df[params_col].apply(pd.Series)\n",
    "    # params_df = params_df.dropna()\n",
    "    # Concatenate the expanded parameters with the original DataFrame\n",
    "    filtered_df = pd.concat([filtered_df.drop(columns=[params_col]), params_df], axis=1)\n",
    "    \n",
    "    return filtered_df, params_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_statistics(df, target_col, predictor_cols):\n",
    "    \"\"\"\n",
    "    Analyzes feature relevance using statistical associations with the target column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame containing the data\n",
    "    - target_col: String, name of the target column\n",
    "    - predictor_cols: List of strings, names of predictor columns\n",
    "\n",
    "    Returns:\n",
    "    - feature_scores: DataFrame with statistical association scores\n",
    "    - optimal_values: Dictionary mapping features to optimal values (based on max target mean)\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=[target_col])\n",
    "    categorical_cols = [col for col in predictor_cols if df[col].dtype == 'object']\n",
    "    numeric_cols = [col for col in predictor_cols if col not in categorical_cols]\n",
    "\n",
    "    feature_scores = []\n",
    "    optimal_values = {}\n",
    "\n",
    "    # Handle numeric features\n",
    "    for col in numeric_cols:\n",
    "        temp_df = df[[col, target_col]].dropna()\n",
    "        if temp_df.empty:\n",
    "            continue\n",
    "        corr = temp_df[col].corr(temp_df[target_col])\n",
    "        grouped = temp_df.groupby(col)[target_col].mean()\n",
    "        if not grouped.empty:\n",
    "            opt_val = grouped.idxmax()\n",
    "        else:\n",
    "            opt_val = None\n",
    "        feature_scores.append({'Feature': col, 'Score': abs(corr), 'Type': 'Numeric'})\n",
    "        optimal_values[col] = opt_val\n",
    "\n",
    "    # Handle categorical features\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype(str).fillna(\"Missing\")\n",
    "        grouped = df.groupby(col)[target_col].mean()\n",
    "        if not grouped.empty:\n",
    "            opt_val = grouped.idxmax()\n",
    "            score = grouped.max() - grouped.min()\n",
    "        else:\n",
    "            opt_val = None\n",
    "            score = 0\n",
    "        feature_scores.append({'Feature': col, 'Score': score, 'Type': 'Categorical'})\n",
    "        optimal_values[col] = opt_val\n",
    "\n",
    "    feature_scores_df = pd.DataFrame(feature_scores).sort_values(by='Score', ascending=False)\n",
    "\n",
    "    return feature_scores_df, optimal_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 'ARI'\n",
    "bertopic_result_df[f'mean_{score}'] = bertopic_result_df.groupby(['reduction_method', 'cluster_method', 'reduction_params', 'cluster_params'])[f'{score}'].transform('mean')\n",
    "feature_importance, optimal_ranges,optimal_scores = analyze_feature_importance(bertopic_result_df, f'mean_{score}',['reduction_method',\t'cluster_method'], plot_violin=True)\n",
    "\n",
    "print(optimal_ranges)\n",
    "optimal_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare parameters for any given Mmthod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param,cols = filter_and_expand(bertopic_result_df, 'reduction', 'UMAP')\n",
    "df_param1 = df_param\n",
    "feature_importance, optimal_ranges, optimal_scores = analyze_feature_importance(df_param, f'mean_{score}',cols, plot_violin=True)\n",
    "print(optimal_scores)\n",
    "print(optimal_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param,cols = filter_and_expand(bertopic_result_df, 'reduction', 'PHATE')\n",
    "df_param1 = df_param\n",
    "feature_importance, optimal_ranges, optimal_scores = analyze_feature_importance(df_param, f'mean_{score}',cols, plot_violin=True)\n",
    "print(optimal_scores)\n",
    "print(optimal_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_feature_statistics(df_param,f'mean_{score}',cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
