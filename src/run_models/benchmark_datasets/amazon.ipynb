{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "# Set the target folder name you want to reach\n",
    "target_folder = \"phate-for-text\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Loop to move up the directory tree until we reach the target folder\n",
    "while os.path.basename(current_dir) != target_folder:\n",
    "    parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "    if parent_dir == current_dir:\n",
    "        # If we reach the root directory and haven't found the target, exit\n",
    "        raise FileNotFoundError(f\"{target_folder} not found in the directory tree.\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "# Change the working directory to the folder where \"phate-for-text\" is found\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Add the \"phate-for-text\" directory to sys.path\n",
    "sys.path.insert(0, current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Standard Libraries\n",
    "# ===================\n",
    "import importlib\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===================\n",
    "# Data Manipulation\n",
    "# ===================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================\n",
    "# Dimensionality Reduction\n",
    "# ==========================\n",
    "import phate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ========================\n",
    "# Clustering\n",
    "# ========================\n",
    "from hdbscan import HDBSCAN\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from custom_packages.diffusion_condensation import DiffusionCondensation as dc\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Evaluation Metrics\n",
    "# ======================\n",
    "from custom_packages.fowlkes_mallows import FowlkesMallows\n",
    "from sklearn.metrics import adjusted_rand_score, rand_score\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "# ==============\n",
    "# Global Config\n",
    "# ==============\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reload modules if needed\n",
    "importlib.reload(phate)\n",
    "from openai import OpenAI\n",
    "key = os.getenv('GPT_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz_40 = pd.read_csv(\"data/amazon/train_40k.csv\")\n",
    "amz_10 = pd.read_csv(\"data/amazon/val_10k.csv\")\n",
    "\n",
    "amz=pd.concat([amz_40,amz_10])\n",
    "amz = amz.drop_duplicates(subset='Title', keep=False).reset_index(drop=True)\n",
    "amz = amz.drop_duplicates(subset='productId', keep=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz = amz.rename(columns={'Title': 'topic'})\n",
    "amz = amz.rename(columns={'Cat1': 'category_0'})\n",
    "amz = amz.rename(columns={'Cat2': 'category_1'})\n",
    "amz = amz.rename(columns={'Cat3': 'category_2'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz= amz.dropna().reset_index(drop=True)  # remove NaNs\n",
    "amz = amz[amz['topic'].apply(lambda x: isinstance(x, str) and x.strip() != '')].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz.to_csv(\"data/amazon/amz_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14824, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Fetches embeddings using the specified backend: 'gpt' (OpenAI) or 'sentence-transformers'.\n",
    "    \n",
    "    Args:\n",
    "        texts (list of str): List of text inputs.\n",
    "        backend (str): 'gpt' or 'sentence-transformers'.\n",
    "        model (str): Model name for the chosen backend.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of embeddings.\n",
    "    \"\"\"\n",
    " # Make sure `openai` is configured with your API key\n",
    "    batch_size = 200\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Fetching GPT embeddings\", unit=\"batch\"):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        response = client.embeddings.create(input=batch, model=model)\n",
    "        batch_embeddings = [entry.embedding for entry in response.data]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching GPT embeddings:   0%|          | 0/149 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching GPT embeddings: 100%|██████████| 149/149 [02:46<00:00,  1.12s/batch]\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"text-embedding-3-large\" \n",
    "os.makedirs(f'{embedding_model}_results', exist_ok=True)\n",
    "os.makedirs('gpt_embeddings', exist_ok=True)\n",
    "embedding_list = get_embeddings(amz['topic'], model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"gpt_embeddings/amz_embed.npy\",embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list=np.load(\"gpt_embeddings/amz_embed.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{embedding_model}_reduced_embeddings', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.RandomState(seed=42).permutation(len(amz))\n",
    "# Shuffle both documents and embeddings using the same index\n",
    "topic_data = amz.iloc[shuffle_idx].reset_index(drop=True)\n",
    "data = np.array(embedding_list)[shuffle_idx] \n",
    "reverse_idx = np.argsort(shuffle_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "for col in topic_data.columns:\n",
    "    if re.match(r'^category_\\d+$', col): \n",
    "        unique_count = len(topic_data[col].unique())\n",
    "        topic_dict[unique_count] = np.array(topic_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating PHATE...\n",
      "  Running PHATE on 14824 observations and 3072 variables.\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 65.17 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 43.97 seconds.\n",
      "  Calculated graph and diffusion operator in 109.20 seconds.\n",
      "  Calculating landmark operator...\n",
      "    Calculating SVD...\n",
      "    Calculated SVD in 1.41 seconds.\n",
      "    Calculating KMeans...\n",
      "    Calculated KMeans in 2.12 seconds.\n",
      "  Calculated landmark operator in 4.17 seconds.\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 33\n",
      "  Calculated optimal t in 0.76 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.40 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 154.98 seconds.\n",
      "Calculated PHATE in 269.62 seconds.\n"
     ]
    }
   ],
   "source": [
    "reducer_model = phate.PHATE(n_jobs=-2,random_state=42, n_components=300,decay=20,t=\"auto\",n_pca=None) #{'k':10,'alpha':4,'t':3}\n",
    "embed_phate = reducer_model.fit_transform(data)\n",
    "np.save(f\"{embedding_model}_reduced_embeddings/PHATE_amz_embed.npy\",embed_phate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_phate  =np.load(f\"{embedding_model}_reduced_embeddings/PHATE_amz_embed.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth= 3\n",
    "cluster_levels=[]\n",
    "for i in reversed(range(0, depth)):\n",
    "    cluster_levels.append(len(topic_data[f'category_{i}'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "include_pca =True\n",
    "include_umap=True\n",
    "\n",
    "# Load your embeddings\n",
    "embeddings = np.array(data)\n",
    "embedding_methods = {}\n",
    "# PCA to 2D\n",
    "\n",
    "embedding_methods[\"PHATE\"]  =embed_phate\n",
    "if include_pca:\n",
    "    pca = PCA(n_components=300)\n",
    "    embedding_methods[\"PCA\"] = pca.fit_transform(embeddings)\n",
    "np.save(f\"{embedding_model}_reduced_embeddings/PCA_amz_embed.npy\",embedding_methods[\"PCA\"])\n",
    "\n",
    "# # UMAP to 2D\n",
    "if include_umap:\n",
    "    umap_model = umap.UMAP(n_components=300, random_state=42,min_dist=.05,n_neighbors=10)\n",
    "    embedding_methods[\"UMAP\"] = umap_model.fit_transform(embeddings)\n",
    "np.save(f\"{embedding_model}_reduced_embeddings/UMAP_amz_embed_new.npy\",embedding_methods[\"UMAP\"])\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# # # Fit t-SNE\n",
    "# tsne_model = TSNE(n_components=3, random_state=42)\n",
    "# embedding_methods[\"tSNE\"] = tsne_model.fit_transform(embeddings)\n",
    "# np.save(f\"{embedding_model}_reduced_embeddings/tSNE_amz_embed.npy\",embedding_methods[\"tSNE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [39:28<00:00, 789.65s/it]\n"
     ]
    }
   ],
   "source": [
    "scores_all = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for embed_name, embed_data in tqdm(embedding_methods.items()):\n",
    "    for cluster_method in [\"Agglomerative\", \"HDBSCAN\",\"DC\"]:\n",
    "        for level in cluster_levels:\n",
    "            \n",
    "            # Clustering\n",
    "            if cluster_method == \"Agglomerative\":\n",
    "                model = AgglomerativeClustering(n_clusters=level)\n",
    "                model.fit(embed_data)\n",
    "                labels = model.labels_\n",
    "            elif cluster_method == \"HDBSCAN\":\n",
    "                model = hdbscan.HDBSCAN(min_cluster_size=level)\n",
    "                model.fit(embed_data)\n",
    "                labels = model.labels_\n",
    "                Z = model.single_linkage_tree_.to_numpy()\n",
    "                labels = fcluster(Z, i, criterion='maxclust')\n",
    "                labels[labels == -1] = labels.max() + 1\n",
    "            elif cluster_method==\"DC\":\n",
    "                model = dc(min_clusters=level, max_iterations=5000,k=10,alpha=3)\n",
    "                model.fit(embed_data)\n",
    "                labels  =model.labels_\n",
    "                \n",
    "\n",
    "            # Use topic_dict for comparison\n",
    "            available_levels = np.array(sorted(topic_dict.keys()))\n",
    "            # print(available_levels)\n",
    "            closest_level = min(available_levels, key=lambda k: abs(k - level))\n",
    "\n",
    "            topic_series = topic_dict[closest_level]\n",
    "            valid_idx = ~pd.isna(topic_series)\n",
    "\n",
    "            target_lst = topic_series[valid_idx]\n",
    "            label_lst = labels[valid_idx]\n",
    "\n",
    "            # Compute metrics\n",
    "            try:\n",
    "                fm_score = FowlkesMallows.Bk({level: target_lst}, {level: label_lst})[level]['FM']\n",
    "            except:\n",
    "                fm_score = np.nan  # In case of failure\n",
    "\n",
    "            scores_all[(embed_name, cluster_method)][\"FM\"].append(fm_score)\n",
    "            scores_all[(embed_name, cluster_method)][\"Rand\"].append(rand_score(target_lst, label_lst))\n",
    "            scores_all[(embed_name, cluster_method)][\"ARI\"].append(adjusted_rand_score(target_lst, label_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for (embed_name, cluster_method), score_dict in scores_all.items():\n",
    "    n_levels = len(score_dict[\"FM\"])  # assuming all score lists have same length\n",
    "    for i in range(n_levels):\n",
    "        if embed_name == 'UMAP':\n",
    "            rows.append({\n",
    "                \"reduction_method\": embed_name,\n",
    "                \"cluster_method\": cluster_method,\n",
    "                \"level\": cluster_levels[i],  # assumes scores were appended in order\n",
    "                \"FM\": score_dict[\"FM\"][i],\n",
    "                \"Rand\": score_dict[\"Rand\"][i],\n",
    "                \"ARI\": score_dict[\"ARI\"][i],\n",
    "            })\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "scores_df = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: sort for easier viewing\n",
    "scores_df = scores_df.sort_values(by=[\"reduction_method\", \"cluster_method\", \"level\"]).reset_index(drop=True)\n",
    "write_header = not os.path.exists(f'{embedding_model}_results/other_amz_results.csv')\n",
    "scores_df.to_csv(f\"{embedding_model}_results/other_amz_results.csv\",mode='a', index=False, header=write_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"combo_color_map.json\", 'r') as file:\n",
    "        combo_color_map = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['FM', 'Rand', 'ARI']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for (embed_name, method), metric_scores in scores_all.items():\n",
    "        if method==\"DC\":\n",
    "            method=\"Diffusion Condensation\"\n",
    "        combo_key = f\"{embed_name}_{method}\"\n",
    "        plt.plot(\n",
    "            cluster_levels, \n",
    "            metric_scores[metric], \n",
    "            marker='o', \n",
    "            label=f\"{embed_name} {method}\",\n",
    "            color= combo_color_map.get(combo_key, 'black')\n",
    "        )\n",
    "    \n",
    "    plt.title(f\"{metric} Score Across Cluster Levels\")\n",
    "    plt.xlabel(\"Cluster Level\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
