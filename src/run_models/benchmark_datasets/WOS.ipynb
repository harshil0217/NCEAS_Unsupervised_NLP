{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "# Set the target folder name you want to reach\n",
    "target_folder = \"phate-for-text\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Loop to move up the directory tree until we reach the target folder\n",
    "while os.path.basename(current_dir) != target_folder:\n",
    "    parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "    if parent_dir == current_dir:\n",
    "        # If we reach the root directory and haven't found the target, exit\n",
    "        raise FileNotFoundError(f\"{target_folder} not found in the directory tree.\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "# Change the working directory to the folder where \"phate-for-text\" is found\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Add the \"phate-for-text\" directory to sys.path\n",
    "sys.path.insert(0, current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Standard Libraries\n",
    "# ===================\n",
    "import importlib\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "# ===================\n",
    "# Data Manipulation\n",
    "# ===================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================\n",
    "# Dimensionality Reduction\n",
    "# ==========================\n",
    "import phate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# ========================\n",
    "# Clustering\n",
    "# ========================\n",
    "from hdbscan import HDBSCAN\n",
    "import hdbscan\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from custom_packages.diffusion_condensation import DiffusionCondensation as dc\n",
    "\n",
    "# ======================\n",
    "# Evaluation Metrics\n",
    "# ======================\n",
    "from custom_packages.fowlkes_mallows import FowlkesMallows\n",
    "from sklearn.metrics import adjusted_rand_score, rand_score\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "# ==============\n",
    "# Global Config\n",
    "# ==============\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reload modules if needed\n",
    "importlib.reload(phate)\n",
    "from openai import OpenAI\n",
    "key = os.getenv('GPT_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/WebOfScience/Meta-data/Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Y</th>\n",
       "      <th>Domain</th>\n",
       "      <th>area</th>\n",
       "      <th>keywords</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>CS</td>\n",
       "      <td>Symbolic computation</td>\n",
       "      <td>(2+1)-dimensional non-linear optical waves; e...</td>\n",
       "      <td>(2 + 1)-dimensional non-linear optical waves t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y1  Y2   Y Domain                     area  \\\n",
       "0   0  12  12    CS    Symbolic computation     \n",
       "\n",
       "                                            keywords  \\\n",
       "0   (2+1)-dimensional non-linear optical waves; e...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  (2 + 1)-dimensional non-linear optical waves t...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "new=[]\n",
    "for i,row in df.iterrows():\n",
    "    # for phrase in row['keywords'].split(';'):\n",
    "    result={}\n",
    "    result['topic']=str(row['keywords'])\n",
    "    result['category 0'] = row['Domain']\n",
    "    result['category 1'] = row['area']\n",
    "    # result['category 2'] = i\n",
    "    new.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>category 0</th>\n",
       "      <th>category 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2+1)-dimensional non-linear optical waves; e...</td>\n",
       "      <td>CS</td>\n",
       "      <td>Symbolic computation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aging; Tau; Amyloid; PET; Alzheimer's disease...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Alzheimer's Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LED lighting system; PV system; Distributed l...</td>\n",
       "      <td>Civil</td>\n",
       "      <td>Green Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NdFeB magnets; Electric motor; Electric vehic...</td>\n",
       "      <td>ECE</td>\n",
       "      <td>Electric motor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parkinson's disease; dyskinesia; adenosine A(...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Parkinson's Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46980</th>\n",
       "      <td>Karate; Verletzungsrisiko; Sportverletzung; P...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Sports Injuries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46981</th>\n",
       "      <td>Z-Wave; Wireless; Embedded systems; Internet ...</td>\n",
       "      <td>CS</td>\n",
       "      <td>Data structures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46982</th>\n",
       "      <td>Antifouling biosensor; Peptide; Electrochemis...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46983</th>\n",
       "      <td>High Performance Computing; Parallel Computin...</td>\n",
       "      <td>CS</td>\n",
       "      <td>Distributed computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46984</th>\n",
       "      <td>gene expression; MPK3; plant-host interaction...</td>\n",
       "      <td>biochemistry</td>\n",
       "      <td>Molecular biology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46985 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   topic     category 0  \\\n",
       "0       (2+1)-dimensional non-linear optical waves; e...            CS    \n",
       "1       Aging; Tau; Amyloid; PET; Alzheimer's disease...       Medical    \n",
       "2       LED lighting system; PV system; Distributed l...         Civil    \n",
       "3       NdFeB magnets; Electric motor; Electric vehic...           ECE    \n",
       "4       Parkinson's disease; dyskinesia; adenosine A(...       Medical    \n",
       "...                                                  ...            ...   \n",
       "46980   Karate; Verletzungsrisiko; Sportverletzung; P...       Medical    \n",
       "46981   Z-Wave; Wireless; Embedded systems; Internet ...            CS    \n",
       "46982   Antifouling biosensor; Peptide; Electrochemis...       Medical    \n",
       "46983   High Performance Computing; Parallel Computin...            CS    \n",
       "46984   gene expression; MPK3; plant-host interaction...  biochemistry    \n",
       "\n",
       "                     category 1  \n",
       "0        Symbolic computation    \n",
       "1         Alzheimer's Disease    \n",
       "2              Green Building    \n",
       "3              Electric motor    \n",
       "4         Parkinson's Disease    \n",
       "...                         ...  \n",
       "46980         Sports Injuries    \n",
       "46981         Data structures    \n",
       "46982                  Cancer    \n",
       "46983   Distributed computing    \n",
       "46984       Molecular biology    \n",
       "\n",
       "[46985 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Fetches embeddings using the specified backend: 'gpt' (OpenAI) or 'sentence-transformers'.\n",
    "    \n",
    "    Args:\n",
    "        texts (list of str): List of text inputs.\n",
    "        backend (str): 'gpt' or 'sentence-transformers'.\n",
    "        model (str): Model name for the chosen backend.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of embeddings.\n",
    "    \"\"\"\n",
    " # Make sure `openai` is configured with your API key\n",
    "    batch_size = 200\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Fetching GPT embeddings\", unit=\"batch\"):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        response = client.embeddings.create(input=batch, model=model)\n",
    "        batch_embeddings = [entry.embedding for entry in response.data]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"text-embedding-3-large\" \n",
    "os.makedirs(f'{embedding_model}_results', exist_ok=True)\n",
    "os.makedirs('gpt_embeddings', exist_ok=True)\n",
    "embedding_list = get_embeddings(df_new['topic'], model=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{embedding_model}_reduced_embeddings', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"gpt_embeddings/WOS_grouped_embed.npy\",embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = np.load(\"gpt_embeddings/WOS_grouped_embed.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46985, 3072)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.RandomState(seed=42).permutation(len(df_new))\n",
    "# Shuffle both documents and embeddings using the same index\n",
    "topic_data = df_new.iloc[shuffle_idx].reset_index(drop=True)\n",
    "data = np.array(embedding_list)[shuffle_idx]\n",
    "reverse_idx = np.argsort(shuffle_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size = int(0.20 * len(topic_data))\n",
    "# topic_data_sample = topic_data.iloc[:sample_size].reset_index(drop=True)\n",
    "# data_sample = data[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "for col in topic_data.columns:\n",
    "    if re.match(r'^category \\d+$', col): \n",
    "        unique_count = len(topic_data[col].unique())\n",
    "        topic_dict[unique_count] = np.array(topic_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating PHATE...\n",
      "  Running PHATE on 46985 observations and 3072 variables.\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 687.58 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 567.51 seconds.\n",
      "  Calculated graph and diffusion operator in 1255.41 seconds.\n",
      "  Calculating landmark operator...\n",
      "    Calculating SVD...\n",
      "    Calculated SVD in 7.65 seconds.\n",
      "    Calculating KMeans...\n",
      "    Calculated KMeans in 2.90 seconds.\n",
      "  Calculated landmark operator in 11.81 seconds.\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 31\n",
      "  Calculated optimal t in 0.72 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.57 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 149.19 seconds.\n",
      "Calculated PHATE in 1417.88 seconds.\n"
     ]
    }
   ],
   "source": [
    "reducer_model = phate.PHATE(n_jobs=-2,random_state=42, n_components=300,decay=20,t=\"auto\",n_pca=None)\n",
    "embed_phate = reducer_model.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(f\"{embedding_model}_reduced_embeddings/PHATE_WOS_embed.npy\",np.array(embed_phate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_phate = np.load(f\"{embedding_model}_reduced_embeddings/PHATE_WOS_embed.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth= 2\n",
    "cluster_levels=[]\n",
    "for i in reversed(range(0, depth)):\n",
    "    cluster_levels.append(len(topic_data[f'category {i}'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_pca =True\n",
    "include_umap=True\n",
    "\n",
    "# Load your embeddings\n",
    "embeddings = np.array(data)\n",
    "embedding_methods = {}\n",
    "# PCA to 2D\n",
    "\n",
    "embedding_methods[\"PHATE\"]  =embed_phate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if include_pca:\n",
    "    pca = PCA(n_components=300)\n",
    "    embedding_methods[\"PCA\"] = pca.fit_transform(embeddings)\n",
    "np.save(f\"{embedding_model}_reduced_embeddings/PCA_WOS_embed.npy\",embedding_methods[\"PCA\"])\n",
    "\n",
    "# # UMAP to 2D\n",
    "if include_umap:\n",
    "    umap_model = umap.UMAP(n_components=300, random_state=42,min_dist=.05,n_neighbors=10)\n",
    "    embedding_methods[\"UMAP\"] = umap_model.fit_transform(embeddings)\n",
    "np.save(f\"{embedding_model}_reduced_embeddings/UMAP_WOS_embed_new.npy\",embedding_methods[\"UMAP\"])\n",
    "\n",
    "# # # Fit t-SNE\n",
    "tsne_model = TSNE(n_components=3, random_state=42)\n",
    "embedding_methods[\"tSNE\"] = tsne_model.fit_transform(embeddings)\n",
    "np.save(f\"{embedding_model}_reduced_embeddings/tSNE_WOS_embed.npy\",embedding_methods[\"tSNE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_methods[\"PCA\"]= np.load(f\"{embedding_model}_reduced_embeddings/PCA_WOS_embed.npy\")\n",
    "\n",
    "embedding_methods[\"UMAP\"] =  np.load(f\"{embedding_model}_reduced_embeddings/UMAP_WOS_embed_new.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_all = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for embed_name, embed_data in embedding_methods.items():\n",
    "    for cluster_method in [\"Agglomerative\", \"HDBSCAN\",\"DC\"]:\n",
    "        for level in cluster_levels:\n",
    "            \n",
    "            # Clustering\n",
    "            if cluster_method == \"Agglomerative\":\n",
    "                model = AgglomerativeClustering(n_clusters=level)\n",
    "                model.fit(embed_data)\n",
    "                labels = model.labels_\n",
    "            elif cluster_method == \"HDBSCAN\":\n",
    "                model = hdbscan.HDBSCAN(min_cluster_size=level)\n",
    "                model.fit(embed_data)\n",
    "                labels = model.labels_\n",
    "                Z = model.single_linkage_tree_.to_numpy()\n",
    "                labels = fcluster(Z, i, criterion='maxclust')\n",
    "                labels[labels == -1] = labels.max() + 1\n",
    "            elif cluster_method==\"DC\":\n",
    "                model = dc(min_clusters=level, max_iterations=5000,k=10,alpha=4,t=3)\n",
    "                model.fit(embed_data)\n",
    "                labels  =model.labels_\n",
    "                \n",
    "\n",
    "            # Use topic_dict for comparison\n",
    "            available_levels = sorted(topic_dict.keys())\n",
    "            closest_level = min(available_levels, key=lambda k: abs(k - level))\n",
    "\n",
    "            topic_series = topic_dict[closest_level]\n",
    "            valid_idx = ~pd.isna(topic_series)\n",
    "\n",
    "            target_lst = topic_series[valid_idx]\n",
    "            label_lst = labels[valid_idx]\n",
    "\n",
    "            # Compute metrics\n",
    "            try:\n",
    "                fm_score = FowlkesMallows.Bk({level: target_lst}, {level: label_lst})[level]['FM']\n",
    "            except:\n",
    "                fm_score = np.nan  # In case of failure\n",
    "\n",
    "            scores_all[(embed_name, cluster_method)][\"FM\"].append(fm_score)\n",
    "            scores_all[(embed_name, cluster_method)][\"Rand\"].append(rand_score(target_lst, label_lst))\n",
    "            scores_all[(embed_name, cluster_method)][\"ARI\"].append(adjusted_rand_score(target_lst, label_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "\n",
    "for (embed_name, cluster_method), score_dict in scores_all.items():\n",
    "    n_levels = len(score_dict[\"FM\"])  # assuming all score lists have same length\n",
    "    for i in range(n_levels):\n",
    "        rows.append({\n",
    "            \"reduction_method\": embed_name,\n",
    "            \"cluster_method\": cluster_method,\n",
    "            \"level\": cluster_levels[i],  # assumes scores were appended in order\n",
    "            \"FM\": score_dict[\"FM\"][i],\n",
    "            \"Rand\": score_dict[\"Rand\"][i],\n",
    "            \"ARI\": score_dict[\"ARI\"][i],\n",
    "            \"Params\":\"{'k':10,'alpha':4,'t':3}\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "scores_df = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: sort for easier viewing\n",
    "scores_df = scores_df.sort_values(by=[\"reduction_method\", \"cluster_method\", \"level\"]).reset_index(drop=True)\n",
    "write_header = not os.path.exists(f'{embedding_model}_results/other_WOS_results.csv')\n",
    "scores_df.to_csv(f\"{embedding_model}_results/other_WOS_results.csv\",mode='a', index=False, header=write_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"combo_color_map.json\", 'r') as file:\n",
    "        combo_color_map = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['FM', 'Rand', 'ARI']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for (embed_name, method), metric_scores in scores_all.items():\n",
    "        if method==\"DC\":\n",
    "            method=\"Diffusion Condensation\"\n",
    "        combo_key = f\"{embed_name}_{method}\"\n",
    "        plt.plot(\n",
    "            cluster_levels, \n",
    "            metric_scores[metric], \n",
    "            marker='o', \n",
    "            label=f\"{embed_name} {method}\",\n",
    "            color= combo_color_map.get(combo_key, 'black')\n",
    "        )\n",
    "    \n",
    "    plt.title(f\"{metric} Score Across Cluster Levels\")\n",
    "    plt.xlabel(\"Cluster Level\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
