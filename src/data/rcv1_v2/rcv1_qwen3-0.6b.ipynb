{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e270b6-dca8-46d1-ac7a-3698ae0777dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory set to: /home/raosidha/CMSE_495\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Hardcoded to my CMSE_495 directory structure\n",
    "repo_root = \"/home/raosidha/CMSE_495\"\n",
    "\n",
    "os.chdir(repo_root)\n",
    "sys.path.insert(0, repo_root)\n",
    "sys.path.insert(0, os.path.join(repo_root, \"src\"))\n",
    "\n",
    "print(f\"Directory set to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c730115-56a0-40e3-a735-68fc42b7f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Loaded 5000 documents.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import phate, umap\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, rand_score\n",
    "from hdbscan import HDBSCAN\n",
    "from custom_packages.diffusion_condensation import DiffusionCondensation as dc\n",
    "from custom_packages.fowlkes_mallows import FowlkesMallows\n",
    "\n",
    "# Using the paths that worked in your environment\n",
    "metadata_path = \"src/data/rcv1_v2/src_2/data_2/rcv1_v2/rcv1_qwen_metadata.csv\"\n",
    "embeddings_path = \"src/data/rcv1_v2/src_2/data_2/rcv1_v2/rcv1_qwen_embeddings.npy\"\n",
    "\n",
    "df = pd.read_csv(metadata_path)\n",
    "data = np.load(embeddings_path)\n",
    "print(f\"Success! Loaded {data.shape[0]} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17266c8-e1ef-43ec-81d1-94ee1b53dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA, UMAP, and PHATE...\n",
      "Calculating PHATE...\n",
      "  Running PHATE on 5000 observations and 1024 variables.\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating PCA...\n",
      "    Calculated PCA in 1.63 seconds.\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 4.89 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 1.73 seconds.\n",
      "  Calculated graph and diffusion operator in 8.26 seconds.\n",
      "  Calculating landmark operator...\n",
      "    Calculating SVD...\n",
      "    Calculated SVD in 3.97 seconds.\n",
      "    Calculating KMeans...\n",
      "    Calculated KMeans in 17.00 seconds.\n",
      "  Calculated landmark operator in 20.96 seconds.\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 35\n",
      "  Calculated optimal t in 20.21 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 1.58 seconds.\n",
      "  Calculating metric MDS...\n",
      "    SGD-MDS may not have converged: stress changed by 1.2% in final iterations. Consider increasing n_iter or adjusting learning_rate.\n",
      "  Calculated metric MDS in 4.00 seconds.\n",
      "Calculated PHATE in 55.70 seconds.\n",
      "Starting clustering suite...\n",
      "  Processing PCA...\n",
      "  Processing UMAP...\n",
      "  Processing PHATE...\n",
      "Final Release Results saved successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Running PCA, UMAP, and PHATE...\")\n",
    "reductions = {\n",
    "    \"PCA\": PCA(n_components=50, random_state=42).fit_transform(data),\n",
    "    \"UMAP\": umap.UMAP(n_components=2, random_state=42).fit_transform(data),\n",
    "    \"PHATE\": phate.PHATE(n_components=3, random_state=42).fit_transform(data)\n",
    "}\n",
    "\n",
    "results = []\n",
    "levels = [\"category 0\", \"category 1\", \"category 2\"]\n",
    "\n",
    "print(\"Starting clustering suite...\")\n",
    "for red_name, red_data in reductions.items():\n",
    "    print(f\"  Processing {red_name}...\")\n",
    "    for method in [\"Agglomerative\", \"HDBSCAN\", \"DC\"]:\n",
    "        for i, col in enumerate(levels):\n",
    "            # Target number of clusters for this hierarchy level\n",
    "            n_clusters = len(df[col].unique())\n",
    "            \n",
    "            if method == \"Agglomerative\":\n",
    "                model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "                lbls = model.fit_predict(red_data)\n",
    "                \n",
    "            elif method == \"HDBSCAN\":\n",
    "                # Aligning with ArXiv logic to get exactly n_clusters\n",
    "                model = HDBSCAN(min_cluster_size=5)\n",
    "                model.fit(red_data)\n",
    "                Z = model.single_linkage_tree_.to_numpy()\n",
    "                lbls = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "                # Handle noise points if any\n",
    "                lbls[lbls == -1] = lbls.max() + 1\n",
    "                \n",
    "            elif method == \"DC\":\n",
    "                # FIX: Separate fit from label access to avoid 2D array error\n",
    "                model_dc = dc(\n",
    "                    min_clusters=n_clusters, \n",
    "                    max_iterations=5000, \n",
    "                    k=10, \n",
    "                    alpha=3\n",
    "                )\n",
    "                model_dc.fit(red_data)\n",
    "                lbls = model_dc.labels_ \n",
    "            \n",
    "            # Metrics Calculation\n",
    "            target = df[col]\n",
    "            results.append({\n",
    "                \"reduction_method\": red_name,\n",
    "                \"cluster_method\": method,\n",
    "                \"level\": i,\n",
    "                \"ARI\": adjusted_rand_score(target, lbls),\n",
    "                \"Rand\": rand_score(target, lbls),\n",
    "                \"FM\": FowlkesMallows.Bk({i: target}, {i: lbls})[i][\"FM\"]\n",
    "            })\n",
    "\n",
    "# Save Final Results\n",
    "os.makedirs(\"Qwen3-Embedding-0.6B_results\", exist_ok=True)\n",
    "scores_df = pd.DataFrame(results)\n",
    "scores_df.to_csv(\"Qwen3-Embedding-0.6B_results/rcv1_results.csv\", index=False)\n",
    "print(\"Final Release Results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f0763-398d-4c34-92cb-07c2e87ceeca",
   "metadata": {},
   "source": [
    "reduction_method,cluster_method,level,ARI,Rand,FM\n",
    "\n",
    "PCA,Agglomerative,0,0.1716996411770845,0.8314942188437687,0.32100305892801256\n",
    "\n",
    "PCA,Agglomerative,1,0.1716996411770845,0.8314942188437687,0.32100305892801256\n",
    "\n",
    "PCA,Agglomerative,2,0.1716996411770845,0.8314942188437687,0.32100305892801256\n",
    "\n",
    "PCA,HDBSCAN,0,0.009609517143094356,0.21238095619123826,0.43573385155534045\n",
    "\n",
    "PCA,HDBSCAN,1,0.009609517143094356,0.21238095619123826,0.43573385155534045\n",
    "\n",
    "PCA,HDBSCAN,2,0.009609517143094356,0.21238095619123826,0.43573385155534045\n",
    "\n",
    "PCA,DC,0,0.12649173269201136,0.7885168233646729,0.24763318814349114\n",
    "\n",
    "PCA,DC,1,0.12649173269201136,0.7885168233646729,0.24763318814349114\n",
    "\n",
    "PCA,DC,2,0.12649173269201136,0.7885168233646729,0.24763318814349114\n",
    "\n",
    "Our initial benchmark on the RCV1 dataset using PCA and Qwen3-0.6B embeddings shows that Agglomerative Clustering is the most effective current method with an ARI of 0.17. We observed identical scores across hierarchy levels because the specific data subset used contains documents where the root and leaf categories are identical. Future work will focus on using PHATE (which is still running) to see if it better separates these topics than PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279a3a3-87fc-4a44-acec-24217bf0fe65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
