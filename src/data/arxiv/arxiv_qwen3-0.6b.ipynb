{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to repo root\n",
    "target_folder = \"NCEAS_Unsupervised_NLP\"\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "while os.path.basename(current_dir) != target_folder:\n",
    "    parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "    if parent_dir == current_dir:\n",
    "        raise FileNotFoundError(f\"{target_folder} not found.\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Add repo root\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "# Add src so custom_packages works\n",
    "sys.path.insert(0, os.path.join(current_dir, \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# Standard Imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import phate\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, rand_score\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from custom_packages.diffusion_condensation import DiffusionCondensation as dc\n",
    "from custom_packages.fowlkes_mallows import FowlkesMallows\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load arXiv Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"src/data/arxiv/data/arxiv/arxiv_30k_clean.csv\")\n",
    "df_new = pd.DataFrame()\n",
    "df_new[\"topic\"] = df[\"text\"]\n",
    "df_new[\"category_1\"] = df[\"label\"]\n",
    "df_new[\"category_0\"] = df[\"label\"].apply(lambda x: x.split(\".\")[0])\n",
    "\n",
    "df_new = df_new.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.dropna().reset_index(drop=True)\n",
    "\n",
    "df_new = df_new[\n",
    "    df_new[\"topic\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
    "].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"src/data/arxiv/arxiv_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (30000, 1024)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"arxiv\"\n",
    "embedding_model = \"Qwen3-Embedding-0.6B\"\n",
    "\n",
    "embed_path = \"src/data/arxiv/arxiv_qwen_embeddings.npy\"\n",
    "\n",
    "embedding_list = np.load(embed_path)\n",
    "\n",
    "print(\"Embedding shape:\", embedding_list.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after shuffle: (30000, 1024)\n"
     ]
    }
   ],
   "source": [
    "embedding_list.shape[0] == len(df_new)\n",
    "shuffle_idx = np.random.RandomState(seed=42).permutation(len(df_new))\n",
    "# Shuffle both documents and embeddings using the same index\n",
    "topic_data = df_new.iloc[shuffle_idx].reset_index(drop=True)\n",
    "data = np.array(embedding_list)[shuffle_idx] \n",
    "reverse_idx = np.argsort(shuffle_idx)\n",
    "print(\"Data shape after shuffle:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'category_1', 'category_0'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(topic_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "for col in topic_data.columns:\n",
    "    if re.match(r'^category_\\d+$', col): \n",
    "        unique_count = len(topic_data[col].unique())\n",
    "        topic_dict[unique_count] = np.array(topic_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating PHATE...\n",
      "  Running PHATE on 30000 observations and 1024 variables.\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 15.22 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 18.04 seconds.\n",
      "  Calculated graph and diffusion operator in 33.54 seconds.\n",
      "  Calculating landmark operator...\n",
      "    Calculating SVD...\n",
      "    Calculated SVD in 13.32 seconds.\n",
      "    Calculating KMeans...\n",
      "    Calculated KMeans in 3.64 seconds.\n",
      "  Calculated landmark operator in 16.96 seconds.\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 19\n",
      "  Calculated optimal t in 2.88 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.64 seconds.\n",
      "  Calculating metric MDS...\n",
      "    SGD-MDS may not have converged: stress changed by -3.9% in final iterations. Consider increasing n_iter or adjusting learning_rate.\n",
      "  Calculated metric MDS in 86.91 seconds.\n",
      "Calculated PHATE in 142.89 seconds.\n"
     ]
    }
   ],
   "source": [
    "reducer_model = phate.PHATE(n_jobs=-2,random_state=42, n_components=300,decay=20,t=\"auto\",n_pca=None) #{'k':10,'alpha':4,'t':3}\n",
    "embed_phate = reducer_model.fit_transform(data)\n",
    "np.save(\n",
    "    f\"{embedding_model}_reduced_embeddings/PHATE_{dataset_name}_embed.npy\",\n",
    "    embed_phate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_phate = np.load(\n",
    "    f\"{embedding_model}_reduced_embeddings/PHATE_arxiv_embed.npy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Build Hierarchy Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth= 2\n",
    "cluster_levels=[]\n",
    "for i in reversed(range(0, depth)):\n",
    "    cluster_levels.append(len(topic_data[f'category_{i}'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "include_pca = True\n",
    "include_umap = True\n",
    "\n",
    "# Convert embeddings\n",
    "embeddings = np.array(data)\n",
    "\n",
    "embedding_methods = {}\n",
    "\n",
    "# =====================\n",
    "# PHATE (already computed)\n",
    "# =====================\n",
    "embedding_methods[\"PHATE\"] = embed_phate\n",
    "\n",
    "np.save(\n",
    "    f\"{embedding_model}_reduced_embeddings/PHATE_{dataset_name}_embed.npy\",\n",
    "    embedding_methods[\"PHATE\"]\n",
    ")\n",
    "\n",
    "# =====================\n",
    "# PCA\n",
    "# =====================\n",
    "if include_pca:\n",
    "    pca = PCA(n_components=50, random_state=42)\n",
    "    embedding_methods[\"PCA\"] = pca.fit_transform(embeddings)\n",
    "\n",
    "    np.save(\n",
    "        f\"{embedding_model}_reduced_embeddings/PCA_{dataset_name}_embed.npy\",\n",
    "        embedding_methods[\"PCA\"]\n",
    "    )\n",
    "\n",
    "# =====================\n",
    "# UMAP\n",
    "# =====================\n",
    "if include_umap:\n",
    "    umap_model = umap.UMAP(\n",
    "        n_components=50,\n",
    "        random_state=42,\n",
    "        min_dist=0.05,\n",
    "        n_neighbors=10\n",
    "    )\n",
    "\n",
    "    embedding_methods[\"UMAP\"] = umap_model.fit_transform(embeddings)\n",
    "\n",
    "    np.save(\n",
    "        f\"{embedding_model}_reduced_embeddings/UMAP_{dataset_name}_embed.npy\",\n",
    "        embedding_methods[\"UMAP\"]\n",
    "    )\n",
    "\n",
    "# =====================\n",
    "# Optional: t-SNE\n",
    "# =====================\n",
    "# from sklearn.manifold import TSNE\n",
    "# tsne_model = TSNE(n_components=3, random_state=42)\n",
    "# embedding_methods[\"tSNE\"] = tsne_model.fit_transform(embeddings)\n",
    "# np.save(\n",
    "#     f\"{embedding_model}_reduced_embeddings/tSNE_{dataset_name}_embed.npy\",\n",
    "#     embedding_methods[\"tSNE\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mhdbscan\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m scores_all = \u001b[43mdefaultdict\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m embed_name, embed_data \u001b[38;5;129;01min\u001b[39;00m tqdm(embedding_methods.items()):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m cluster_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mAgglomerative\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHDBSCAN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDC\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mNameError\u001b[39m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "scores_all = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for embed_name, embed_data in tqdm(embedding_methods.items()):\n",
    "    for cluster_method in [\"Agglomerative\", \"HDBSCAN\", \"DC\"]:\n",
    "        for level in cluster_levels:\n",
    "\n",
    "            # -----------------\n",
    "            # Clustering\n",
    "            # -----------------\n",
    "            if cluster_method == \"Agglomerative\":\n",
    "                model = AgglomerativeClustering(n_clusters=level)\n",
    "                model.fit(embed_data)\n",
    "                labels = model.labels_\n",
    "\n",
    "            elif cluster_method == \"HDBSCAN\":\n",
    "                model = HDBSCAN(min_cluster_size=level)\n",
    "                model.fit(embed_data)\n",
    "\n",
    "                Z = model.single_linkage_tree_.to_numpy()\n",
    "                labels = fcluster(Z, level, criterion='maxclust')\n",
    "                labels[labels == -1] = labels.max() + 1\n",
    "\n",
    "            elif cluster_method == \"DC\":\n",
    "                model = dc(min_clusters=level, max_iterations=5000, k=10, alpha=3)\n",
    "                model.fit(embed_data)\n",
    "                labels = model.labels_\n",
    "\n",
    "            # -----------------\n",
    "            # Match ground truth\n",
    "            # -----------------\n",
    "            available_levels = np.array(sorted(topic_dict.keys()))\n",
    "            closest_level = min(available_levels, key=lambda k: abs(k - level))\n",
    "\n",
    "            topic_series = topic_dict[closest_level]\n",
    "            valid_idx = ~pd.isna(topic_series)\n",
    "\n",
    "            target_lst = topic_series[valid_idx]\n",
    "            label_lst = labels[valid_idx]\n",
    "\n",
    "            # -----------------\n",
    "            # Metrics\n",
    "            # -----------------\n",
    "            try:\n",
    "                fm_score = FowlkesMallows.Bk(\n",
    "                    {level: target_lst},\n",
    "                    {level: label_lst}\n",
    "                )[level][\"FM\"]\n",
    "            except:\n",
    "                fm_score = np.nan\n",
    "\n",
    "            scores_all[(embed_name, cluster_method)][\"FM\"].append(fm_score)\n",
    "            scores_all[(embed_name, cluster_method)][\"Rand\"].append(\n",
    "                rand_score(target_lst, label_lst)\n",
    "            )\n",
    "            scores_all[(embed_name, cluster_method)][\"ARI\"].append(\n",
    "                adjusted_rand_score(target_lst, label_lst)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for (embed_name, cluster_method), score_dict in scores_all.items():\n",
    "    n_levels = len(score_dict[\"FM\"])\n",
    "\n",
    "    for i in range(n_levels):\n",
    "        rows.append({\n",
    "            \"reduction_method\": embed_name,\n",
    "            \"cluster_method\": cluster_method,\n",
    "            \"level\": cluster_levels[i],\n",
    "            \"FM\": score_dict[\"FM\"][i],\n",
    "            \"Rand\": score_dict[\"Rand\"][i],\n",
    "            \"ARI\": score_dict[\"ARI\"][i],\n",
    "        })\n",
    "\n",
    "scores_df = pd.DataFrame(rows)\n",
    "\n",
    "scores_df = scores_df.sort_values(\n",
    "    by=[\"reduction_method\", \"cluster_method\", \"level\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "write_header = not os.path.exists(\n",
    "    f\"{embedding_model}_results/other_{dataset_name}_results.csv\"\n",
    ")\n",
    "\n",
    "scores_df.to_csv(\n",
    "    f\"{embedding_model}_results/other_{dataset_name}_results.csv\",\n",
    "    mode=\"a\",\n",
    "    index=False,\n",
    "    header=write_header\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"combo_color_map.json\", 'r') as file:\n",
    "        combo_color_map = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['FM', 'Rand', 'ARI']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for (embed_name, method), metric_scores in scores_all.items():\n",
    "        if method==\"DC\":\n",
    "            method=\"Diffusion Condensation\"\n",
    "        combo_key = f\"{embed_name}_{method}\"\n",
    "        plt.plot(\n",
    "            cluster_levels, \n",
    "            metric_scores[metric], \n",
    "            marker='o', \n",
    "            label=f\"{embed_name} {method}\",\n",
    "            color= combo_color_map.get(combo_key, 'black')\n",
    "        )\n",
    "    \n",
    "    plt.title(f\"{metric} Score Across Cluster Levels\")\n",
    "    plt.xlabel(\"Cluster Level\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
