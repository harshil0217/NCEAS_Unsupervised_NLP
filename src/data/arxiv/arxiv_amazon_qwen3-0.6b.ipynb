{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to repo root\n",
    "target_folder = \"NCEAS_Unsupervised_NLP\"\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "while os.path.basename(current_dir) != target_folder:\n",
    "    parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "    if parent_dir == current_dir:\n",
    "        raise FileNotFoundError(f\"{target_folder} not found.\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Add repo root\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "# Add src so custom_packages works\n",
    "sys.path.insert(0, os.path.join(current_dir, \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# Standard Imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import phate\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, rand_score\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from custom_packages.diffusion_condensation import DiffusionCondensation as dc\n",
    "from custom_packages.fowlkes_mallows import FowlkesMallows\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load arXiv Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"src/data/arxiv/data/arxiv/arxiv_30k_clean.csv\")\n",
    "df_new = pd.DataFrame()\n",
    "df_new[\"topic\"] = df[\"text\"]\n",
    "df_new[\"category_1\"] = df[\"label\"]\n",
    "df_new[\"category_0\"] = df[\"label\"].apply(lambda x: x.split(\".\")[0])\n",
    "\n",
    "df_new = df_new.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.dropna().reset_index(drop=True)\n",
    "\n",
    "df_new = df_new[\n",
    "    df_new[\"topic\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
    "].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"src/data/arxiv/arxiv_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"arxiv\"\n",
    "embedding_model = \"Qwen3-Embedding-0.6B\"\n",
    "\n",
    "os.makedirs(\"qwen_embeddings\", exist_ok=True)\n",
    "\n",
    "embed_path = f\"qwen_embeddings/{dataset_name}_embed.npy\"\n",
    "\n",
    "if os.path.exists(embed_path):\n",
    "    print(\"Loading existing embeddings...\")\n",
    "    embedding_list = np.load(embed_path)\n",
    "else:\n",
    "    print(\"Generating embeddings...\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", device=device)\n",
    "\n",
    "    embedding_list = model.encode(\n",
    "        df_new[\"topic\"].tolist(),\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    np.save(embed_path, embedding_list)\n",
    "\n",
    "print(\"Embedding shape:\", embedding_list.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Shuffle both documents and embeddings using the same index\u001b[39;00m\n\u001b[32m      3\u001b[39m topic_data = df_new.iloc[shuffle_idx].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data = np.array(\u001b[43membedding_list\u001b[49m)[shuffle_idx] \n\u001b[32m      5\u001b[39m reverse_idx = np.argsort(shuffle_idx)\n",
      "\u001b[31mNameError\u001b[39m: name 'embedding_list' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_list.shape[0] == len(df_new)\n",
    "shuffle_idx = np.random.RandomState(seed=42).permutation(len(df_new))\n",
    "# Shuffle both documents and embeddings using the same index\n",
    "topic_data = df_new.iloc[shuffle_idx].reset_index(drop=True)\n",
    "data = np.array(embedding_list)[shuffle_idx] \n",
    "reverse_idx = np.argsort(shuffle_idx)\n",
    "print(\"Data shape after shuffle:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'category_1', 'category_0'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(topic_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "for col in topic_data.columns:\n",
    "    if re.match(r'^category_\\d+$', col): \n",
    "        unique_count = len(topic_data[col].unique())\n",
    "        topic_dict[unique_count] = np.array(topic_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating PHATE...\n",
      "  Running PHATE on 14824 observations and 3072 variables.\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 65.17 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 43.97 seconds.\n",
      "  Calculated graph and diffusion operator in 109.20 seconds.\n",
      "  Calculating landmark operator...\n",
      "    Calculating SVD...\n",
      "    Calculated SVD in 1.41 seconds.\n",
      "    Calculating KMeans...\n",
      "    Calculated KMeans in 2.12 seconds.\n",
      "  Calculated landmark operator in 4.17 seconds.\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 33\n",
      "  Calculated optimal t in 0.76 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.40 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 154.98 seconds.\n",
      "Calculated PHATE in 269.62 seconds.\n"
     ]
    }
   ],
   "source": [
    "reducer_model = phate.PHATE(n_jobs=-2,random_state=42, n_components=300,decay=20,t=\"auto\",n_pca=None) #{'k':10,'alpha':4,'t':3}\n",
    "embed_phate = reducer_model.fit_transform(data)\n",
    "np.save(\n",
    "    f\"{embedding_model}_reduced_embeddings/PHATE_{dataset_name}_embed.npy\",\n",
    "    embed_phate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_phate = np.load(\n",
    "    f\"{embedding_model}_reduced_embeddings/PHATE_amz_embed.npy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth= 3\n",
    "cluster_levels=[]\n",
    "for i in reversed(range(0, depth)):\n",
    "    cluster_levels.append(len(topic_data[f'category_{i}'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "include_pca = True\n",
    "include_umap = True\n",
    "\n",
    "# Convert embeddings\n",
    "embeddings = np.array(data)\n",
    "\n",
    "embedding_methods = {}\n",
    "\n",
    "# =====================\n",
    "# PHATE (already computed)\n",
    "# =====================\n",
    "embedding_methods[\"PHATE\"] = embed_phate\n",
    "\n",
    "np.save(\n",
    "    f\"{embedding_model}_reduced_embeddings/PHATE_{dataset_name}_embed.npy\",\n",
    "    embedding_methods[\"PHATE\"]\n",
    ")\n",
    "\n",
    "# =====================\n",
    "# PCA\n",
    "# =====================\n",
    "if include_pca:\n",
    "    pca = PCA(n_components=300, random_state=42)\n",
    "    embedding_methods[\"PCA\"] = pca.fit_transform(embeddings)\n",
    "\n",
    "    np.save(\n",
    "        f\"{embedding_model}_reduced_embeddings/PCA_{dataset_name}_embed.npy\",\n",
    "        embedding_methods[\"PCA\"]\n",
    "    )\n",
    "\n",
    "# =====================\n",
    "# UMAP\n",
    "# =====================\n",
    "if include_umap:\n",
    "    umap_model = umap.UMAP(\n",
    "        n_components=300,\n",
    "        random_state=42,\n",
    "        min_dist=0.05,\n",
    "        n_neighbors=10\n",
    "    )\n",
    "\n",
    "    embedding_methods[\"UMAP\"] = umap_model.fit_transform(embeddings)\n",
    "\n",
    "    np.save(\n",
    "        f\"{embedding_model}_reduced_embeddings/UMAP_{dataset_name}_embed.npy\",\n",
    "        embedding_methods[\"UMAP\"]\n",
    "    )\n",
    "\n",
    "# =====================\n",
    "# Optional: t-SNE\n",
    "# =====================\n",
    "# from sklearn.manifold import TSNE\n",
    "# tsne_model = TSNE(n_components=3, random_state=42)\n",
    "# embedding_methods[\"tSNE\"] = tsne_model.fit_transform(embeddings)\n",
    "# np.save(\n",
    "#     f\"{embedding_model}_reduced_embeddings/tSNE_{dataset_name}_embed.npy\",\n",
    "#     embedding_methods[\"tSNE\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [39:28<00:00, 789.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "scores_all = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for embed_name, embed_data in tqdm(embedding_methods.items()):\n",
    "    for cluster_method in [\"Agglomerative\", \"HDBSCAN\", \"DC\"]:\n",
    "        for level in cluster_levels:\n",
    "\n",
    "            # -----------------\n",
    "            # Clustering\n",
    "            # -----------------\n",
    "            if cluster_method == \"Agglomerative\":\n",
    "                model = AgglomerativeClustering(n_clusters=level)\n",
    "                model.fit(embed_data)\n",
    "                labels = model.labels_\n",
    "\n",
    "            elif cluster_method == \"HDBSCAN\":\n",
    "                model = HDBSCAN(min_cluster_size=level)\n",
    "                model.fit(embed_data)\n",
    "\n",
    "                Z = model.single_linkage_tree_.to_numpy()\n",
    "                labels = fcluster(Z, level, criterion='maxclust')\n",
    "                labels[labels == -1] = labels.max() + 1\n",
    "\n",
    "            elif cluster_method == \"DC\":\n",
    "                model = dc(min_clusters=level, max_iterations=5000, k=10, alpha=3)\n",
    "                model.fit(embed_data)\n",
    "                labels = model.labels_\n",
    "\n",
    "            # -----------------\n",
    "            # Match ground truth\n",
    "            # -----------------\n",
    "            available_levels = np.array(sorted(topic_dict.keys()))\n",
    "            closest_level = min(available_levels, key=lambda k: abs(k - level))\n",
    "\n",
    "            topic_series = topic_dict[closest_level]\n",
    "            valid_idx = ~pd.isna(topic_series)\n",
    "\n",
    "            target_lst = topic_series[valid_idx]\n",
    "            label_lst = labels[valid_idx]\n",
    "\n",
    "            # -----------------\n",
    "            # Metrics\n",
    "            # -----------------\n",
    "            try:\n",
    "                fm_score = FowlkesMallows.Bk(\n",
    "                    {level: target_lst},\n",
    "                    {level: label_lst}\n",
    "                )[level][\"FM\"]\n",
    "            except:\n",
    "                fm_score = np.nan\n",
    "\n",
    "            scores_all[(embed_name, cluster_method)][\"FM\"].append(fm_score)\n",
    "            scores_all[(embed_name, cluster_method)][\"Rand\"].append(\n",
    "                rand_score(target_lst, label_lst)\n",
    "            )\n",
    "            scores_all[(embed_name, cluster_method)][\"ARI\"].append(\n",
    "                adjusted_rand_score(target_lst, label_lst)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for (embed_name, cluster_method), score_dict in scores_all.items():\n",
    "    n_levels = len(score_dict[\"FM\"])\n",
    "\n",
    "    for i in range(n_levels):\n",
    "        rows.append({\n",
    "            \"reduction_method\": embed_name,\n",
    "            \"cluster_method\": cluster_method,\n",
    "            \"level\": cluster_levels[i],\n",
    "            \"FM\": score_dict[\"FM\"][i],\n",
    "            \"Rand\": score_dict[\"Rand\"][i],\n",
    "            \"ARI\": score_dict[\"ARI\"][i],\n",
    "        })\n",
    "\n",
    "scores_df = pd.DataFrame(rows)\n",
    "\n",
    "scores_df = scores_df.sort_values(\n",
    "    by=[\"reduction_method\", \"cluster_method\", \"level\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "write_header = not os.path.exists(\n",
    "    f\"{embedding_model}_results/other_{dataset_name}_results.csv\"\n",
    ")\n",
    "\n",
    "scores_df.to_csv(\n",
    "    f\"{embedding_model}_results/other_{dataset_name}_results.csv\",\n",
    "    mode=\"a\",\n",
    "    index=False,\n",
    "    header=write_header\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"combo_color_map.json\", 'r') as file:\n",
    "        combo_color_map = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['FM', 'Rand', 'ARI']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for (embed_name, method), metric_scores in scores_all.items():\n",
    "        if method==\"DC\":\n",
    "            method=\"Diffusion Condensation\"\n",
    "        combo_key = f\"{embed_name}_{method}\"\n",
    "        plt.plot(\n",
    "            cluster_levels, \n",
    "            metric_scores[metric], \n",
    "            marker='o', \n",
    "            label=f\"{embed_name} {method}\",\n",
    "            color= combo_color_map.get(combo_key, 'black')\n",
    "        )\n",
    "    \n",
    "    plt.title(f\"{metric} Score Across Cluster Levels\")\n",
    "    plt.xlabel(\"Cluster Level\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
