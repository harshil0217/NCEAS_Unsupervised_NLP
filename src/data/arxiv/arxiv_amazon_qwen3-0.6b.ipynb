{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to repo root\n",
    "target_folder = \"NCEAS_Unsupervised_NLP\"\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "while os.path.basename(current_dir) != target_folder:\n",
    "    parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "    if parent_dir == current_dir:\n",
    "        raise FileNotFoundError(f\"{target_folder} not found.\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Add repo root\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "# Add src so custom_packages works\n",
    "sys.path.insert(0, os.path.join(current_dir, \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# Standard Imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import phate\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, rand_score\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from custom_packages.diffusion_condensation import DiffusionCondensation as dc\n",
    "from custom_packages.fowlkes_mallows import FowlkesMallows\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "# Set the target folder name you want to reach\n",
    "target_folder = \"NCEAS_Unsupervised_NLP\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Loop to move up the directory tree until we reach the target folder\n",
    "while os.path.basename(current_dir) != target_folder:\n",
    "    parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "    if parent_dir == current_dir:\n",
    "        # If we reach the root directory and haven't found the target, exit\n",
    "        raise FileNotFoundError(f\"{target_folder} not found in the directory tree.\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "# Change the working directory to the folder where \"phate-for-text\" is found\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Add the \"phate-for-text\" directory to sys.path\n",
    "sys.path.insert(0, current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to Python path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"src\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to repo root\n",
    "target_folder = \"NCEAS_Unsupervised_NLP\"\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "while os.path.basename(current_dir) != target_folder:\n",
    "    parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "    if parent_dir == current_dir:\n",
    "        raise FileNotFoundError(f\"{target_folder} not found.\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Add repo root\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "# Add src so custom_packages works\n",
    "sys.path.insert(0, os.path.join(current_dir, \"src\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'phate' from '/opt/anaconda3/lib/python3.12/site-packages/phate/__init__.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================\n",
    "# Standard Libraries\n",
    "# ===================\n",
    "import importlib\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===================\n",
    "# Data Manipulation\n",
    "# ===================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================\n",
    "# Dimensionality Reduction\n",
    "# ==========================\n",
    "import phate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ========================\n",
    "# Clustering\n",
    "# ========================\n",
    "from hdbscan import HDBSCAN\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from custom_packages.diffusion_condensation import DiffusionCondensation as dc\n",
    "\n",
    "# ======================\n",
    "# Evaluation Metrics\n",
    "# ======================\n",
    "from custom_packages.fowlkes_mallows import FowlkesMallows\n",
    "from sklearn.metrics import adjusted_rand_score, rand_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(phate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"src/data/arxiv/data/arxiv/arxiv_30k_clean.csv\")\n",
    "df_new = pd.DataFrame()\n",
    "df_new[\"topic\"] = df[\"text\"]\n",
    "df_new[\"category_1\"] = df[\"label\"]\n",
    "df_new[\"category_0\"] = df[\"label\"].apply(lambda x: x.split(\".\")[0])\n",
    "\n",
    "df_new = df_new.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.dropna().reset_index(drop=True)\n",
    "\n",
    "df_new = df_new[\n",
    "    df_new[\"topic\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
    "].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"src/data/arxiv/arxiv_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def get_embeddings(texts, model_name=\"Qwen/Qwen3-Embedding-0.6B\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bda40f83b61481199d28e3d485294fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c079afab5a4379a1d00b85f01289b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"arxiv\"\n",
    "embedding_model = \"Qwen3-Embedding-0.6B\"\n",
    "\n",
    "os.makedirs(f\"{embedding_model}_results\", exist_ok=True)\n",
    "os.makedirs(\"qwen_embeddings\", exist_ok=True)\n",
    "\n",
    "embedding_list = get_embeddings(df_new[\"topic\"].tolist())\n",
    "\n",
    "np.save(f\"qwen_embeddings/{dataset_name}_embed.npy\", embedding_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m embedding_model = \u001b[33m\"\u001b[39m\u001b[33mqwen\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33mqwen_embeddings\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m np.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mqwen_embeddings/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_embed.npy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         \u001b[43membedding_list\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'embedding_list' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_name = \"arxiv\"\n",
    "embedding_model = \"qwen\"\n",
    "\n",
    "os.makedirs(\"qwen_embeddings\", exist_ok=True)\n",
    "\n",
    "np.save(f\"qwen_embeddings/{dataset_name}_{embedding_model}_embed.npy\",\n",
    "        embedding_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{embedding_model}_reduced_embeddings', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Shuffle both documents and embeddings using the same index\u001b[39;00m\n\u001b[32m      3\u001b[39m topic_data = df_new.iloc[shuffle_idx].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data = np.array(\u001b[43membedding_list\u001b[49m)[shuffle_idx] \n\u001b[32m      5\u001b[39m reverse_idx = np.argsort(shuffle_idx)\n",
      "\u001b[31mNameError\u001b[39m: name 'embedding_list' is not defined"
     ]
    }
   ],
   "source": [
    "shuffle_idx = np.random.RandomState(seed=42).permutation(len(df_new))\n",
    "# Shuffle both documents and embeddings using the same index\n",
    "topic_data = df_new.iloc[shuffle_idx].reset_index(drop=True)\n",
    "data = np.array(embedding_list)[shuffle_idx] \n",
    "reverse_idx = np.argsort(shuffle_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'category_1', 'category_0'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(topic_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "for col in topic_data.columns:\n",
    "    if re.match(r'^category_\\d+$', col): \n",
    "        unique_count = len(topic_data[col].unique())\n",
    "        topic_dict[unique_count] = np.array(topic_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating PHATE...\n",
      "  Running PHATE on 14824 observations and 3072 variables.\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 65.17 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 43.97 seconds.\n",
      "  Calculated graph and diffusion operator in 109.20 seconds.\n",
      "  Calculating landmark operator...\n",
      "    Calculating SVD...\n",
      "    Calculated SVD in 1.41 seconds.\n",
      "    Calculating KMeans...\n",
      "    Calculated KMeans in 2.12 seconds.\n",
      "  Calculated landmark operator in 4.17 seconds.\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 33\n",
      "  Calculated optimal t in 0.76 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.40 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 154.98 seconds.\n",
      "Calculated PHATE in 269.62 seconds.\n"
     ]
    }
   ],
   "source": [
    "reducer_model = phate.PHATE(n_jobs=-2,random_state=42, n_components=300,decay=20,t=\"auto\",n_pca=None) #{'k':10,'alpha':4,'t':3}\n",
    "embed_phate = reducer_model.fit_transform(data)\n",
    "np.save(\n",
    "    f\"{embedding_model}_reduced_embeddings/PHATE_{dataset_name}_embed.npy\",\n",
    "    embed_phate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_phate = np.load(\n",
    "    f\"{embedding_model}_reduced_embeddings/PHATE_amz_embed.npy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth= 3\n",
    "cluster_levels=[]\n",
    "for i in reversed(range(0, depth)):\n",
    "    cluster_levels.append(len(topic_data[f'category_{i}'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "include_pca = True\n",
    "include_umap = True\n",
    "\n",
    "# Convert embeddings\n",
    "embeddings = np.array(data)\n",
    "\n",
    "embedding_methods = {}\n",
    "\n",
    "# =====================\n",
    "# PHATE (already computed)\n",
    "# =====================\n",
    "embedding_methods[\"PHATE\"] = embed_phate\n",
    "\n",
    "np.save(\n",
    "    f\"{embedding_model}_reduced_embeddings/PHATE_{dataset_name}_embed.npy\",\n",
    "    embedding_methods[\"PHATE\"]\n",
    ")\n",
    "\n",
    "# =====================\n",
    "# PCA\n",
    "# =====================\n",
    "if include_pca:\n",
    "    pca = PCA(n_components=300, random_state=42)\n",
    "    embedding_methods[\"PCA\"] = pca.fit_transform(embeddings)\n",
    "\n",
    "    np.save(\n",
    "        f\"{embedding_model}_reduced_embeddings/PCA_{dataset_name}_embed.npy\",\n",
    "        embedding_methods[\"PCA\"]\n",
    "    )\n",
    "\n",
    "# =====================\n",
    "# UMAP\n",
    "# =====================\n",
    "if include_umap:\n",
    "    umap_model = umap.UMAP(\n",
    "        n_components=300,\n",
    "        random_state=42,\n",
    "        min_dist=0.05,\n",
    "        n_neighbors=10\n",
    "    )\n",
    "\n",
    "    embedding_methods[\"UMAP\"] = umap_model.fit_transform(embeddings)\n",
    "\n",
    "    np.save(\n",
    "        f\"{embedding_model}_reduced_embeddings/UMAP_{dataset_name}_embed.npy\",\n",
    "        embedding_methods[\"UMAP\"]\n",
    "    )\n",
    "\n",
    "# =====================\n",
    "# Optional: t-SNE\n",
    "# =====================\n",
    "# from sklearn.manifold import TSNE\n",
    "# tsne_model = TSNE(n_components=3, random_state=42)\n",
    "# embedding_methods[\"tSNE\"] = tsne_model.fit_transform(embeddings)\n",
    "# np.save(\n",
    "#     f\"{embedding_model}_reduced_embeddings/tSNE_{dataset_name}_embed.npy\",\n",
    "#     embedding_methods[\"tSNE\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [39:28<00:00, 789.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "scores_all = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for embed_name, embed_data in tqdm(embedding_methods.items()):\n",
    "    for cluster_method in [\"Agglomerative\", \"HDBSCAN\", \"DC\"]:\n",
    "        for level in cluster_levels:\n",
    "\n",
    "            # -----------------\n",
    "            # Clustering\n",
    "            # -----------------\n",
    "            if cluster_method == \"Agglomerative\":\n",
    "                model = AgglomerativeClustering(n_clusters=level)\n",
    "                model.fit(embed_data)\n",
    "                labels = model.labels_\n",
    "\n",
    "            elif cluster_method == \"HDBSCAN\":\n",
    "                model = HDBSCAN(min_cluster_size=level)\n",
    "                model.fit(embed_data)\n",
    "\n",
    "                Z = model.single_linkage_tree_.to_numpy()\n",
    "                labels = fcluster(Z, level, criterion='maxclust')\n",
    "                labels[labels == -1] = labels.max() + 1\n",
    "\n",
    "            elif cluster_method == \"DC\":\n",
    "                model = dc(min_clusters=level, max_iterations=5000, k=10, alpha=3)\n",
    "                model.fit(embed_data)\n",
    "                labels = model.labels_\n",
    "\n",
    "            # -----------------\n",
    "            # Match ground truth\n",
    "            # -----------------\n",
    "            available_levels = np.array(sorted(topic_dict.keys()))\n",
    "            closest_level = min(available_levels, key=lambda k: abs(k - level))\n",
    "\n",
    "            topic_series = topic_dict[closest_level]\n",
    "            valid_idx = ~pd.isna(topic_series)\n",
    "\n",
    "            target_lst = topic_series[valid_idx]\n",
    "            label_lst = labels[valid_idx]\n",
    "\n",
    "            # -----------------\n",
    "            # Metrics\n",
    "            # -----------------\n",
    "            try:\n",
    "                fm_score = FowlkesMallows.Bk(\n",
    "                    {level: target_lst},\n",
    "                    {level: label_lst}\n",
    "                )[level][\"FM\"]\n",
    "            except:\n",
    "                fm_score = np.nan\n",
    "\n",
    "            scores_all[(embed_name, cluster_method)][\"FM\"].append(fm_score)\n",
    "            scores_all[(embed_name, cluster_method)][\"Rand\"].append(\n",
    "                rand_score(target_lst, label_lst)\n",
    "            )\n",
    "            scores_all[(embed_name, cluster_method)][\"ARI\"].append(\n",
    "                adjusted_rand_score(target_lst, label_lst)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for (embed_name, cluster_method), score_dict in scores_all.items():\n",
    "    n_levels = len(score_dict[\"FM\"])\n",
    "\n",
    "    for i in range(n_levels):\n",
    "        rows.append({\n",
    "            \"reduction_method\": embed_name,\n",
    "            \"cluster_method\": cluster_method,\n",
    "            \"level\": cluster_levels[i],\n",
    "            \"FM\": score_dict[\"FM\"][i],\n",
    "            \"Rand\": score_dict[\"Rand\"][i],\n",
    "            \"ARI\": score_dict[\"ARI\"][i],\n",
    "        })\n",
    "\n",
    "scores_df = pd.DataFrame(rows)\n",
    "\n",
    "scores_df = scores_df.sort_values(\n",
    "    by=[\"reduction_method\", \"cluster_method\", \"level\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "write_header = not os.path.exists(\n",
    "    f\"{embedding_model}_results/other_{dataset_name}_results.csv\"\n",
    ")\n",
    "\n",
    "scores_df.to_csv(\n",
    "    f\"{embedding_model}_results/other_{dataset_name}_results.csv\",\n",
    "    mode=\"a\",\n",
    "    index=False,\n",
    "    header=write_header\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"combo_color_map.json\", 'r') as file:\n",
    "        combo_color_map = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['FM', 'Rand', 'ARI']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for (embed_name, method), metric_scores in scores_all.items():\n",
    "        if method==\"DC\":\n",
    "            method=\"Diffusion Condensation\"\n",
    "        combo_key = f\"{embed_name}_{method}\"\n",
    "        plt.plot(\n",
    "            cluster_levels, \n",
    "            metric_scores[metric], \n",
    "            marker='o', \n",
    "            label=f\"{embed_name} {method}\",\n",
    "            color= combo_color_map.get(combo_key, 'black')\n",
    "        )\n",
    "    \n",
    "    plt.title(f\"{metric} Score Across Cluster Levels\")\n",
    "    plt.xlabel(\"Cluster Level\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
